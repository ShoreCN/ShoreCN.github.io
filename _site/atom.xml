<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.7.4">Jekyll</generator><link href="http://localhost:4000/atom.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2019-02-28T11:20:28+08:00</updated><id>http://localhost:4000/atom.xml</id><title type="html">Shore Space</title><subtitle>C/Python/通信/SDN/云计算</subtitle><author><name>Shore</name></author><entry><title type="html">在AWS Ubuntu服务器上使用Docker快速部署Jekyll静态博客</title><link href="http://localhost:4000/%E5%B7%A5%E5%85%B7/2019/02/26/Deploy-Jekyll-by-docker-in-AWS/" rel="alternate" type="text/html" title="在AWS Ubuntu服务器上使用Docker快速部署Jekyll静态博客" /><published>2019-02-26T06:46:49+08:00</published><updated>2019-02-26T06:46:49+08:00</updated><id>http://localhost:4000/%E5%B7%A5%E5%85%B7/2019/02/26/Deploy-Jekyll-by-docker-in-AWS</id><content type="html" xml:base="http://localhost:4000/%E5%B7%A5%E5%85%B7/2019/02/26/Deploy-Jekyll-by-docker-in-AWS/">&lt;h2 id=&quot;jekyll简介&quot;&gt;Jekyll简介&lt;/h2&gt;
&lt;p&gt;Jekyll 是一个简单且完全免费的静态博客站点生成工具，类似WordPress。但是和WordPress又有很大的不同，原因是jekyll只是一个只用 Markdown (或 Textile)、Liquid、HTML &amp;amp; CSS 就可以生成静态网页的工具，不需要数据库支持。但是可以配合第三方服务,例如Disqus。&lt;/p&gt;

&lt;h2 id=&quot;docker简介&quot;&gt;Docker简介&lt;/h2&gt;
&lt;p&gt;Docker是一个开放源代码软件项目，让应用程序部署的工作可以自动化进行，借此在Linux操作系统上，提供一个额外的软件抽象层，以及操作系统层虚拟化的自动管理机制。&lt;/p&gt;

&lt;h2 id=&quot;jekyll部署&quot;&gt;Jekyll部署&lt;/h2&gt;
&lt;h3 id=&quot;环境&quot;&gt;环境&lt;/h3&gt;
&lt;p&gt;AWS Ubuntu 16.04.5 LTS&lt;/p&gt;

&lt;h3 id=&quot;安装&quot;&gt;安装&lt;/h3&gt;
&lt;h4 id=&quot;docker安装&quot;&gt;Docker安装&lt;/h4&gt;
&lt;p&gt;Docker的安装资料已经十分丰富了，参见官网指导即可。&lt;/p&gt;
&lt;blockquote&gt;
  &lt;table&gt;
    &lt;tbody&gt;
      &lt;tr&gt;
        &lt;td&gt;[Get Docker CE for Ubuntu&lt;/td&gt;
        &lt;td&gt;Docker Documentation](https://docs.docker.com/install/linux/docker-ce/ubuntu/)&lt;/td&gt;
      &lt;/tr&gt;
    &lt;/tbody&gt;
  &lt;/table&gt;
&lt;/blockquote&gt;

&lt;h4 id=&quot;jekyll安装&quot;&gt;Jekyll安装&lt;/h4&gt;
&lt;p&gt;因为使用Docker，所以免去了Jekyll安装。不用再关心系统版本、依赖组件之类的问题，开箱即用。&lt;/p&gt;

&lt;h3 id=&quot;选择jekyll模板&quot;&gt;选择Jekyll模板&lt;/h3&gt;
&lt;p&gt;网上有大量的现成Jekyll模板可以选择，此处以Jekyll-Now为例。
在服务器上创建一个目录并git clone模板代码：&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;mkdir jekyllBlog
cd jekyllBlog
git clone https://github.com/barryclark/jekyll-now.git .
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;构建&quot;&gt;构建&lt;/h3&gt;
&lt;p&gt;Jekyll提供了一条build命令将模板编译为静态网页，直接在执行如下Docker命令即可：&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;export JEKYLL_VERSION=3.8
docker run --rm --volume=&quot;$PWD:/srv/jekyll&quot; -it jekyll/jekyll:$JEKYLL_VERSION jekyll build
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;~可以看到新生成一个_site目录，就是刚刚构建出来的网页文件~&lt;/p&gt;

&lt;h3 id=&quot;开启服务&quot;&gt;开启服务&lt;/h3&gt;
&lt;p&gt;Jekyll 同时也集成了一个网页服务，通过serve命令即可开启。
&lt;code class=&quot;highlighter-rouge&quot;&gt;docker run --rm --volume=&quot;$PWD:/srv/jekyll&quot; -it -p 4000:4000 jekyll/jekyll:$JEKYLL_VERSION jekyll serve&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;~通过-p参数指定了对外暴露的的服务器端口为4000~&lt;/p&gt;

&lt;p&gt;至此，我们就可以通过服务器的IP加端口（例如http://1.2.3.4:4000）访问刚刚创建好的静态博客系统了。&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;参考资料&lt;br /&gt;
&lt;a href=&quot;https://stackoverflow.com/questions/53470794/setup-jekyll-via-docker&quot;&gt;Setup Jekyll via Docker? - Stack Overflow&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;</content><author><name>Shore</name></author><summary type="html">Jekyll简介 Jekyll 是一个简单且完全免费的静态博客站点生成工具，类似WordPress。但是和WordPress又有很大的不同，原因是jekyll只是一个只用 Markdown (或 Textile)、Liquid、HTML &amp;amp; CSS 就可以生成静态网页的工具，不需要数据库支持。但是可以配合第三方服务,例如Disqus。</summary></entry><entry><title type="html">使用NGINX Ingress进行Kubernetes集群负载均衡</title><link href="http://localhost:4000/2018/12/25/k8s-nginx-ingress-install/" rel="alternate" type="text/html" title="使用NGINX Ingress进行Kubernetes集群负载均衡" /><published>2018-12-25T08:23:23+08:00</published><updated>2018-12-25T08:23:23+08:00</updated><id>http://localhost:4000/2018/12/25/k8s-nginx-ingress-install</id><content type="html" xml:base="http://localhost:4000/2018/12/25/k8s-nginx-ingress-install/">&lt;h2 id=&quot;1-概述&quot;&gt;1 概述&lt;/h2&gt;
&lt;p&gt;Nginx Ingress Controller 是 Kubernetes Ingress Controller 的一种实现，作为反向代理将外部流量导入集群内部，实现将 Kubernetes 内部的 Service 暴露给外部，这样我们就能通过公网或内网直接访问集群内部的服务。&lt;/p&gt;

&lt;h2 id=&quot;2-安装&quot;&gt;2 安装&lt;/h2&gt;
&lt;p&gt;Ingress的安装方式有两种&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;通过Kubernetes配置文件安装&lt;br /&gt;
Ingress相关的yaml文件归档在如下地址：&lt;a href=&quot;https://github.com/nginxinc/kubernetes-ingress/tree/master/deployments&quot;&gt;kubernetes-ingress/deployments at master · nginxinc/kubernetes-ingress · GitHub&lt;/a&gt;
可以通过下载相关文件后使用kubectl apply直接进行部署，参阅指导文档&lt;a href=&quot;https://github.com/nginxinc/kubernetes-ingress/blob/master/docs/installation.md&quot;&gt;kubernetes-ingress/installation.md at master · nginxinc/kubernetes-ingress · GitHub&lt;/a&gt;，此处不予赘述&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;通过Helm安装&lt;br /&gt;
本文主要介绍通过Helm安装使用Ingress，所以开始前请确保已经安装好Helm了。&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;21-选择服务暴露方式&quot;&gt;2.1 选择服务暴露方式&lt;/h3&gt;
&lt;p&gt;Ingress对外部提供服务通常选择如下两种方式：&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;LoadBalancer&lt;br /&gt;
需要云厂商支持并购买，为每个LoadBalancer类型的Service分配公网IP地址&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;hostPort&lt;br /&gt;
hostPort是直接将容器的端口与所调度的节点上的端口连接起来，这样用户就可以通过宿主机的IP加上端口号来访问Pod了，如:&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;apiVersion: v1
kind: Pod
metadata:
  name: influxdb
spec:
  containers:
    - name: influxdb
      image: influxdb
      ports:
        - containerPort: 8086
          hostPort: 8086
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;blockquote&gt;
  &lt;p&gt;hostPort的方案有个缺点，因为Pod重新调度的时候该Pod被调度到的宿主机可能会变动，这样就变化后用户必须自己维护一个Pod与所在宿主机的对应关系。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h3 id=&quot;22-使用loadbalancer提供服务&quot;&gt;2.2 使用LoadBalancer提供服务&lt;/h3&gt;
&lt;p&gt;这种方式部署起来十分简单，因为stable/nginx-ingress这个helm包默认就是使用这种方式部署。确保在云厂商上已经开通了LoadBalancer服务之后，执行如下命令进行一键安装：&lt;br /&gt;
&lt;code class=&quot;highlighter-rouge&quot;&gt;helm install --name nginx-ingress --namespace kube-system stable/nginx-ingress&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;部署完成后查看结果：&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;$ kubectl get svc -n kube-system
NAME                            TYPE           CLUSTER-IP     EXTERNAL-IP      PORT(S)                      AGE
nginx-ingress-controller        LoadBalancer   10.3.255.138   119.28.121.125   80:30113/TCP,443:32564/TCP   21h
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;23-使用hostport提供服务&quot;&gt;2.3 使用hostPort提供服务&lt;/h3&gt;
&lt;p&gt;之前有提到hostPort的缺点在于Pod被调度变化到其他宿主机时需要手动维护。那么我们通过DaemonSet将Nginx 的 Ingress Controller Pod指定到特定宿主机上面来解决这个问题，牺牲了一定的可靠性，但就可以不再操心如何手工维护服务的地址变化问题了，二者的利弊取舍可以根据使用场景进行评估。&lt;/p&gt;

&lt;h4 id=&quot;231-选择边缘节点&quot;&gt;2.3.1 选择边缘节点&lt;/h4&gt;
&lt;p&gt;边缘节点：监听外部流量进入集群内部的节点
这里我们选择集群内部的一个节点作为边缘节点，注意这个节点需要有外部可以访问的公网地址并且80、443端口没有被其他应用占用。&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;[root@k8s-cluster1-master ~]# kubectl get nodes
NAME                      STATUS   ROLES    AGE   VERSION
izj6cfiv0zhc0j2k7ruyhsz   Ready    &amp;lt;none&amp;gt;   16d   v1.13.0
k8s-cluster1-master       Ready    master   16d   v1.13.0
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;以这个集群为例，我们选择izj6cfiv0zhc0j2k7ruyhsz作为边缘节点&lt;/p&gt;

&lt;h4 id=&quot;232-为边缘节点添加标签&quot;&gt;2.3.2 为边缘节点添加标签&lt;/h4&gt;
&lt;p&gt;我们使用如下指令给这个节点加上一个内容为node:edge的label，后续在DaemonSet部署时根据label进行绑定即可。&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;$ kubectl label node izj6cfiv0zhc0j2k7ruyhsz node=edge
node &quot;izj6cfiv0zhc0j2k7ruyhsz&quot; labeled
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;删除标签可以使用如下命令&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;$ kubectl label node izj6cfiv0zhc0j2k7ruyhsz node-
node &quot;izj6cfiv0zhc0j2k7ruyhsz&quot; labeled
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h4 id=&quot;233-使用helm安装ingress&quot;&gt;2.3.3 使用helm安装ingress&lt;/h4&gt;
&lt;p&gt;配置好边缘节点之后即可以使用Helm安装ingress了，安装命令如下：&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;helm install stable/nginx-ingress \
  --namespace kube-system \
  --name nginx-ingress \
  --set controller.kind=DaemonSet \
  --set controller.daemonset.useHostPort=true \
  --set controller.nodeSelector.node=edge \
  --set controller.service.type=ClusterIP
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;参数解释：&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;namespace  可选项，配置后可指定ingress安装的命令空间，默认为default&lt;/li&gt;
  &lt;li&gt;name  生成的资源名称&lt;/li&gt;
  &lt;li&gt;set controller.kind=DaemonSet  选择部署方式为DaemonSet&lt;/li&gt;
  &lt;li&gt;set controller.daemonset.useHostPort=true  开启hostPort功能，可以直接通过节点访问ingress服务&lt;/li&gt;
  &lt;li&gt;controller.nodeSelector.node=edge  提供对外提供服务的节点筛选方式，需要和之前配置的节点label匹配，例如之前配置的node=edge，这里也需要填上一样的键值&lt;/li&gt;
  &lt;li&gt;set controller.service.type=ClusterIP  内部服务的类型&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;安装完成之后检查一下对应pod是否都正常运行了&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;[root@k8s-cluster1-master ~]# kubectl get pods -n kube-system | grep nginx-ingress
nginx-ingress-controller-hv8h9                   1/1     Running   0          15d
nginx-ingress-default-backend-56d99b86fb-4jlxn   1/1     Running   0          15d
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;3-配置&quot;&gt;3 配置&lt;/h2&gt;
&lt;h3 id=&quot;31-创建测试服务&quot;&gt;3.1 创建测试服务&lt;/h3&gt;
&lt;p&gt;Ingress安装完成之后，就可以配置相应规则来对外暴露服务了。我们可以创建一个服务来测试
创建一个my-nginx.yaml，内容为&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;  apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: my-nginx
spec:
  replicas: 1
  template:
    metadata:
      labels:
        run: my-nginx
    spec:
      containers:
      - name: my-nginx
        image: nginx
        ports:
        - containerPort: 80
---
apiVersion: v1
kind: Service
metadata:
  name: my-nginx
  labels:
    app: my-nginx
spec:
  ports:
  - port: 80
    protocol: TCP
    name: http
  selector:
    run: my-nginx
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;通过&lt;code class=&quot;highlighter-rouge&quot;&gt;kubectl apply -f my-nginx.yaml&lt;/code&gt;进行创建，通过&lt;code class=&quot;highlighter-rouge&quot;&gt;kubectl get service&lt;/code&gt;可以看到my-nginx服务已经创建成功&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;[root@k8s-cluster1-master ~]# kubectl get svc
NAME                                 TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)          AGE
kubernetes                           ClusterIP   10.96.0.1        &amp;lt;none&amp;gt;        443/TCP          17d
my-nginx                             ClusterIP   10.100.181.157   &amp;lt;none&amp;gt;        80/TCP,443/TCP   13d
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;32-创建ingress规则&quot;&gt;3.2 创建ingress规则&lt;/h3&gt;
&lt;p&gt;创建完成服务之后，我们就可以通过配置ingress规则来暴露服务了，&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: my-nginx
  annotations:
    kubernetes.io/ingress.class: &quot;nginx&quot;
    nginx.ingress.kubernetes.io/rewrite-target: /
spec:
  rules:
  - host: www.10cloud.tk
    http:
      paths:
      - path: /web
        backend:
          serviceName: my-nginx
          servicePort: 80
  tls:
  - hosts:
    - www.10cloud.tk
    secretName: www-10cloud-tk-tls
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;参数解释&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;kubernetes.io/ingress.class: “nginx”&lt;br /&gt;
通过定义kubernetes.io/ingress.class 这个annotation可以在有多个ingress controller的情况下能够让请求被我们安装的这个处理&lt;/li&gt;
  &lt;li&gt;nginx.ingress.kubernetes.io/rewrite-target: /&lt;br /&gt;
配置位置重定向，作用是使Ingress以根路径转发到后端，避免访问路径错误配置而导致的404错误。参数详细说明参照&lt;a href=&quot;https://github.com/kubernetes/ingress-nginx/tree/master/docs/examples/rewrite&quot;&gt;ingress-nginx/docs/examples/rewrite at master · kubernetes/ingress-nginx · GitHub&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;host&lt;br /&gt;
域名地址&lt;/li&gt;
  &lt;li&gt;path&lt;br /&gt;
外部访问的相对地址&lt;/li&gt;
  &lt;li&gt;backend&lt;br /&gt;
通过配置服务名称和服务端口来指定和path绑定的后端服务&lt;/li&gt;
  &lt;li&gt;tls&lt;br /&gt;
创建了证书之后，可以通过tls绑定已创建的secret来实现https安全访问功能&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;至此，我们的Ingress创建完成并且通过配置服务与域名的关系，实现了外部访问的功能，通过域名&lt;code class=&quot;highlighter-rouge&quot;&gt;chenshuo.tk/web&lt;/code&gt;即看到nginx的内容，说明通过域名已经可以访问集群内的服务了&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Welcome to nginx!
If you see this page, the nginx web server is successfully installed and working. Further configuration is required.

For online documentation and support please refer to nginx.org.
Commercial support is available at nginx.com.

Thank you for using nginx.
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;</content><author><name>Shore</name></author><category term="Kubernetes" /><summary type="html">1 概述 Nginx Ingress Controller 是 Kubernetes Ingress Controller 的一种实现，作为反向代理将外部流量导入集群内部，实现将 Kubernetes 内部的 Service 暴露给外部，这样我们就能通过公网或内网直接访问集群内部的服务。</summary></entry><entry><title type="html">Linux系统下Helm（Kubernetes包管理器）的安装使用</title><link href="http://localhost:4000/kubernetes/2018/12/11/Helm-install/" rel="alternate" type="text/html" title="Linux系统下Helm（Kubernetes包管理器）的安装使用" /><published>2018-12-11T19:14:39+08:00</published><updated>2018-12-11T19:14:39+08:00</updated><id>http://localhost:4000/kubernetes/2018/12/11/Helm-install</id><content type="html" xml:base="http://localhost:4000/kubernetes/2018/12/11/Helm-install/">&lt;h2 id=&quot;概述&quot;&gt;概述&lt;/h2&gt;
&lt;p&gt;Helm 是 Kubernetes 的包管理器，可以帮我们简化 kubernetes 的操作，一键部署应用。假如你的机器上已经安装了 kubectl 并且能够操作集群，那么你就可以安装 Helm 了。&lt;/p&gt;

&lt;h2 id=&quot;安装&quot;&gt;安装&lt;/h2&gt;
&lt;h3 id=&quot;安装客户端helm&quot;&gt;安装客户端（Helm）&lt;/h3&gt;
&lt;p&gt;官方提供了多种安装方式，这里选用脚本安装的方法，使用命令&lt;code class=&quot;highlighter-rouge&quot;&gt;curl https://raw.githubusercontent.com/helm/helm/master/scripts/get | bash&lt;/code&gt;会自动抓取最新的版本完成客户端的安装：&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;[root@k8s-cluster1-master ~]# curl https://raw.githubusercontent.com/helm/helm/master/scripts/get | bash
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
100  7236  100  7236    0     0  74494      0 --:--:-- --:--:-- --:--:-- 74597
Downloading https://kubernetes-helm.storage.googleapis.com/helm-v2.12.0-linux-amd64.tar.gz
Preparing to install helm and tiller into /usr/local/bin
helm installed into /usr/local/bin/helm
tiller installed into /usr/local/bin/tiller
Run 'helm init' to configure helm.
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;安装服务端tiller&quot;&gt;安装服务端（Tiller）&lt;/h3&gt;
&lt;p&gt;Helm安装完成之后，通过命令行&lt;code class=&quot;highlighter-rouge&quot;&gt;helm init&lt;/code&gt;即可完成服务端Tiller的安装。&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;[root@k8s-cluster1-master ~]# helm init
Creating /root/.helm
Creating /root/.helm/repository
Creating /root/.helm/repository/cache
Creating /root/.helm/repository/local
Creating /root/.helm/plugins
Creating /root/.helm/starters
Creating /root/.helm/cache/archive
Creating /root/.helm/repository/repositories.yaml
Adding stable repo with URL: https://kubernetes-charts.storage.googleapis.com
Adding local repo with URL: http://127.0.0.1:8879/charts
$HELM_HOME has been configured at /root/.helm.

Tiller (the Helm server-side component) has been installed into your Kubernetes Cluster.

Please note: by default, Tiller is deployed with an insecure 'allow unauthenticated users' policy.
To prevent this, run `helm init` with the --tiller-tls-verify flag.
For more information on securing your installation see: https://docs.helm.sh/using_helm/#securing-your-helm-installation
Happy Helming!
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;使用命令&lt;code class=&quot;highlighter-rouge&quot;&gt;helm version&lt;/code&gt;可以查看安装是否成功和版本号情况&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;[root@k8s-cluster1-master ~]# helm version
Client: &amp;amp;version.Version{SemVer:&quot;v2.12.0&quot;, GitCommit:&quot;d325d2a9c179b33af1a024cdb5a4472b6288016a&quot;, GitTreeState:&quot;clean&quot;}
Server: &amp;amp;version.Version{SemVer:&quot;v2.12.0&quot;, GitCommit:&quot;d325d2a9c179b33af1a024cdb5a4472b6288016a&quot;, GitTreeState:&quot;clean&quot;}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;权限配置&quot;&gt;权限配置&lt;/h3&gt;
&lt;p&gt;默认安装的 tiller 权限很小，我们执行下面的脚本给它加最大权限，这样方便我们可以用 helm 部署应用到任意 namespace 下:&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;kubectl create serviceaccount --namespace=kube-system tiller

kubectl create clusterrolebinding tiller-cluster-rule --clusterrole=cluster-admin --serviceaccount=kube-system:tiller

kubectl patch deploy --namespace=kube-system tiller-deploy -p '{&quot;spec&quot;:{&quot;template&quot;:{&quot;spec&quot;:{&quot;serviceAccount&quot;:&quot;tiller&quot;}}}}'
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;</content><author><name>Shore</name></author><category term="Kubernetes" /><summary type="html">概述 Helm 是 Kubernetes 的包管理器，可以帮我们简化 kubernetes 的操作，一键部署应用。假如你的机器上已经安装了 kubectl 并且能够操作集群，那么你就可以安装 Helm 了。</summary></entry><entry><title type="html">容器化迁移规范</title><link href="http://localhost:4000/%E5%AE%B9%E5%99%A8/2018/11/28/Container-Trans-Standard/" rel="alternate" type="text/html" title="容器化迁移规范" /><published>2018-11-28T00:00:00+08:00</published><updated>2018-11-28T00:00:00+08:00</updated><id>http://localhost:4000/%E5%AE%B9%E5%99%A8/2018/11/28/Container-Trans-Standard</id><content type="html" xml:base="http://localhost:4000/%E5%AE%B9%E5%99%A8/2018/11/28/Container-Trans-Standard/">&lt;h2 id=&quot;服务器选择&quot;&gt;服务器选择&lt;/h2&gt;
&lt;p&gt;用户自行采购服务器主机，服务器规格要求如下：&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;操作系统：CentOS 7或RHEL 7&lt;/li&gt;
  &lt;li&gt;规格：为了流畅使用Kubernetes功能，主机规格至少达到2 Cpu + 2GB内存&lt;/li&gt;
  &lt;li&gt;区域：不限，根据用户业务自定&lt;/li&gt;
  &lt;li&gt;带宽：不限，根据用户业务自定&lt;/li&gt;
  &lt;li&gt;安全组：需要放开如下Kubernetes集群使用到的端口
    &lt;ul&gt;
      &lt;li&gt;Kubernetes API server：6443*&lt;/li&gt;
      &lt;li&gt;etcd server client API：2379-2380&lt;/li&gt;
      &lt;li&gt;Kubelet API：10250&lt;/li&gt;
      &lt;li&gt;kube-scheduler：10251&lt;/li&gt;
      &lt;li&gt;kube-controller-manager：10252&lt;/li&gt;
      &lt;li&gt;NodePort Services：30000-32767&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;其他：保证主机的MAC地址和product_uuid在集群内是唯一的；服务器主机之间的网络可以完全正常访问&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;集群搭建&quot;&gt;集群搭建&lt;/h2&gt;
&lt;p&gt;集群由至少一台Master加若干Node节点组成，如果需要实现更高可靠性，可以增加更多服务器。Kubernetes提供了较高的灵活性，后续根据业务扩张情况弹性增加服务器也十分方便。
选定好Master之后，即可使用我们提供的工具搭建集群了。&lt;/p&gt;

&lt;h2 id=&quot;应用镜像制作&quot;&gt;应用镜像制作&lt;/h2&gt;
&lt;p&gt;容器化的关键步骤之一就是将应用的部署过程制作成镜像，通过分发镜像完成应用的快速实例化、自动化部署的目的。
制作镜像需要关注如下几个关键要素：&lt;/p&gt;

&lt;h3 id=&quot;系统环境&quot;&gt;系统环境&lt;/h3&gt;
&lt;p&gt;选择应用需要使用的基础镜像，基础镜像通常是各种Linux发行版的Docker镜像比如ubuntu、Debian、centos等，也有已安装好相应软件的基础镜像，如NGINX、Python、redis等。根据用户的需求自行选择对应的系统及版本。
常用的基础镜像可以在官方仓库（https://hub.docker.com/explore）或其他源（&lt;a href=&quot;https://dev.aliyun.com&quot;&gt;开发者平台&lt;/a&gt;）查询获取。&lt;/p&gt;

&lt;h3 id=&quot;应用依赖&quot;&gt;应用依赖&lt;/h3&gt;
&lt;p&gt;通过依赖清单的方式明确的声明所有的依赖项，并且在运行过程中，使用依赖隔离工具来确保程序不会调用系统中存在但清单中未声明的依赖项。这样可以简化环境配置流程，并保证应用的生产和开发环境更加一致。
具体到语言上来说例如Ruby 的 Bundler 使用 Gemfile 作为依赖项声明清单，使用 bundle exec 来进行依赖隔离。Python 中则可分别使用两种工具： Pip 用作依赖声明， Virtualenv 用作依赖隔离。甚至 C 语言也有类似工具， Autoconf 用作依赖声明，静态链接库用作依赖隔离。无论用什么工具，依赖声明和依赖隔离建议一起使用。&lt;/p&gt;

&lt;h3 id=&quot;分离有状态与无状态服务&quot;&gt;分离有状态与无状态服务&lt;/h3&gt;
&lt;p&gt;有状态的服务牵扯到存储卷挂载等问题很难在水平方向（服务器主机层面）上进行迁移，所以任何需要持久化的数据都应存储在后端服务（比如数据库），而非直接和服务端耦合在一起，而且数据库等有状态服务也不适合做进镜像内，制作进镜像的部分应该都为无状态且无共享的。
具体来说，通过使用云数据库或者其他方式，把有状态的模块进行服务化，服务端代码只需要引用地址或访问相应API即可使用相关资源。如此实现之后，服务端镜像化之后可以自由地在各个主机上进行实例化，不受数据持续化等问题的制约。&lt;/p&gt;

&lt;h3 id=&quot;记录部署过程&quot;&gt;记录部署过程&lt;/h3&gt;
&lt;p&gt;进行一次应用部署，并将部署过程中的如下指令操作记录下来：&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;软件安装&lt;/li&gt;
  &lt;li&gt;配置操作&lt;/li&gt;
  &lt;li&gt;选择暴露的端口&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;以下面这个Dockerfile为例：&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;通过ENV指令设置环境变量&lt;/li&gt;
  &lt;li&gt;通过ADD指令将本地文件拷贝到容器里&lt;/li&gt;
  &lt;li&gt;通过RUN指令执行命令行操作，这里主要是yum install进行依赖软件包安装&lt;/li&gt;
  &lt;li&gt;通过EXPOSE指令指定需要暴露的服务端口&lt;/li&gt;
  &lt;li&gt;通过ENTRYPOINT指令设置容器运行起来后需要执行的程序或命令&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;FROM daocloud.io/centos:7 
MAINTAINER mate
 
ENV TZ &quot;Asia/Shanghai&quot; 
ENV TERM xterm 

ADD aliyun-mirror.repo /etc/yum.repos.d/CentOS-Base.repo 
ADD aliyun-epel.repo /etc/yum.repos.d/epel.repo 
RUN yum install -y curl wget tar bzip2 unzip vim-enhanced passwd sudo yum-utils hostname net-tools rsync man &amp;amp;&amp;amp; \ 
	yum install -y gcc gcc-c++ git make automake cmake patch logrotate python-devel libpng-devel libjpeg-devel &amp;amp;&amp;amp; \ 
	yum install -y --enablerepo=epel pwgen python-pip &amp;amp;&amp;amp; \ 
	yum clean all 
RUN pip install supervisor
 
ADD supervisord.conf /etc/supervisord.conf 
RUN mkdir -p /etc/supervisor.conf.d &amp;amp;&amp;amp; mkdir -p /var/log/supervisor 
EXPOSE 22 
ENTRYPOINT [&quot;/usr/bin/supervisord&quot;, &quot;-n&quot;, &quot;-c&quot;, &quot;/etc/supervisord.conf&quot;] 
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;备注：
制作镜像过程中，配置可以通过ENV直接设置到环境变量中，但如果是运行的程序依赖的配置需要文件化，类似样例中的supervisord.conf配置文件，那么此文件最好&lt;/p&gt;

&lt;h3 id=&quot;编写dockerfile&quot;&gt;编写Dockerfile&lt;/h3&gt;
&lt;p&gt;更加详细具体的Dockerfile的语法格式、编写规范可以参考如下资料：&lt;br /&gt;
&lt;a href=&quot;https://docs.docker.com/engine/reference/builder/&quot;&gt;| Docker Documentation&lt;/a&gt;
http://www.docker.org.cn/dockerppt/114.html&lt;/p&gt;</content><author><name>Shore</name></author><summary type="html">服务器选择 用户自行采购服务器主机，服务器规格要求如下： 操作系统：CentOS 7或RHEL 7 规格：为了流畅使用Kubernetes功能，主机规格至少达到2 Cpu + 2GB内存 区域：不限，根据用户业务自定 带宽：不限，根据用户业务自定 安全组：需要放开如下Kubernetes集群使用到的端口 Kubernetes API server：6443* etcd server client API：2379-2380 Kubelet API：10250 kube-scheduler：10251 kube-controller-manager：10252 NodePort Services：30000-32767 其他：保证主机的MAC地址和product_uuid在集群内是唯一的；服务器主机之间的网络可以完全正常访问</summary></entry><entry><title type="html">基于云原生技术的产品开发原则</title><link href="http://localhost:4000/%E6%9E%B6%E6%9E%84/2018/11/25/Cloud-Native-Development-Principle/" rel="alternate" type="text/html" title="基于云原生技术的产品开发原则" /><published>2018-11-25T19:29:39+08:00</published><updated>2018-11-25T19:29:39+08:00</updated><id>http://localhost:4000/%E6%9E%B6%E6%9E%84/2018/11/25/Cloud-Native-Development-Principle</id><content type="html" xml:base="http://localhost:4000/%E6%9E%B6%E6%9E%84/2018/11/25/Cloud-Native-Development-Principle/">&lt;h2 id=&quot;传统开发模式困局&quot;&gt;传统开发模式困局&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;开发、测试、运维无法一体化，上线周期长
开发、测试、线上多套环境需要进行单独维护，容易造成不一致影响各阶段的工作开展，进而引发一些线上故障等问题&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;传统应用开发资源利用率低
考虑业务扩容、峰值等情况，需要冗余一定服务器之类的资源，手工维护这些资源很难做到实时且均衡的水平&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;单体应用架构无法满足应用快速上线和迭代要求
业务代码和系统代码高度耦合造成整体可用性较低，影响产品演进，新功能迭代、问题排查等工作深受制约&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;云原生的三大特征&quot;&gt;云原生的三大特征：&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;容器化封装：以容器为基础，提高整体开发水平，形成代码和组件重用，简化云原生应用程序的维护。在容器中运行应用程序和进程，并作为应用程序部署的独立单元，实现高水平资源隔离。&lt;/li&gt;
  &lt;li&gt;自动化管理：通过集中式的编排调度系统来动态的管理和调度，提高系统和资源利用率降低运维成本。&lt;/li&gt;
  &lt;li&gt;面向微服务：明确服务间的依赖互相解耦，通过松耦合的方式提升应用的整体敏捷性。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;一个优秀的云原生产品，从产品设计阶段就应该考虑这三方面因素，下面逐条进行分析。&lt;/p&gt;

&lt;h2 id=&quot;云原生产品开发原则&quot;&gt;云原生产品开发原则&lt;/h2&gt;
&lt;p&gt;产品容器化过程中需要注意如下几个方面：&lt;/p&gt;
&lt;h3 id=&quot;显示声明依赖关系&quot;&gt;显示声明依赖关系&lt;/h3&gt;
&lt;p&gt;通过依赖清单的方式明确的声明所有的依赖项，并且在运行过程中，使用依赖隔离工具来确保程序不会调用系统中存在但清单中未声明的依赖项。这样可以简化环境配置流程，并保证应用的生产和开发环境更加一致。
具体到语言上来说例如Ruby 的 Bundler 使用 Gemfile 作为依赖项声明清单，使用 bundle exec 来进行依赖隔离。Python 中则可分别使用两种工具： Pip 用作依赖声明， Virtualenv 用作依赖隔离。甚至 C 语言也有类似工具， Autoconf 用作依赖声明，静态链接库用作依赖隔离。无论用什么工具，依赖声明和依赖隔离建议一起使用。&lt;/p&gt;

&lt;h3 id=&quot;分离代码与配置&quot;&gt;分离代码与配置&lt;/h3&gt;
&lt;p&gt;通常，应用的 配置 在不同 部署 (预发布、生产环境、开发环境等等)间会有很大差异。这其中包括：&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;数据库，Memcached，以及其他 后端服务 的配置&lt;/li&gt;
  &lt;li&gt;第三方服务的证书，如 Amazon S3、Twitter 等&lt;/li&gt;
  &lt;li&gt;每份部署特有的配置，如域名等&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;将配置直接以常量的形式硬编码于代码中是一个很不优雅的做法，从便利性上来说这样会导致在不同的场景中需要通过修改代码才能满足需求，从安全性上来说会导致代码开源则会泄露部分关键配置数据。&lt;/p&gt;

&lt;p&gt;常见的两个推荐方案：&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;使用配置文件进行配置管理，坏处是配置文件和代码库还是较容易产生耦合，并且配置格式常常会和开发语言、框架产生依赖&lt;/li&gt;
  &lt;li&gt;使用环境变量进行配置管理，更为推荐的方案，和代码库完全独立并且开发语言框架无关&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;与后端服务保持松耦合的关系&quot;&gt;与后端服务保持松耦合的关系&lt;/h3&gt;
&lt;p&gt;后端服务是指程序运行所需要的通过网络调用的各种服务，如数据库（MySQL，CouchDB），消息/队列系统（RabbitMQ，Beanstalkd），SMTP 邮件发送服务（Postfix），以及缓存系统（Memcached）等等。&lt;/p&gt;

&lt;p&gt;与上一个原则对应，通常建议将后端服务的URL地址或者其他服务定位发现方式存储于配置之中并进行维护，代码直接访问配置即可拿到访问服务的方法，可以做到在不改动代码的情况下，随意的替换各项后端服务。
这样部署可以按需加载或卸载资源。例如，如果应用的数据库服务由于硬件问题出现异常，管理员可以从最近的备份中恢复一个数据库，卸载当前的数据库，然后加载新的数据库 – 整个过程都不需要修改代码，或者是将本地 MySQL 数据库换成第三方服务（例如 Amazon RDS）。
如下是一个简单的产品与后端服务关系的样例图：
&lt;img src=&quot;https://github.com/ShoreCN/ShoreCN.github.io/blob/master/resource/12factor_backing_services.jpg?raw=true&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;以无状态的进程来运行应用&quot;&gt;以无状态的进程来运行应用&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;无状态服务:就是没有特殊状态的服务,各个请求对于服务器来说统一无差别处理,请求自身携带了所有服务端所需要的所有参数(服务端自身不存储跟请求相关的任何数据,不包括数据库存储信息)&lt;/li&gt;
  &lt;li&gt;有状态服务:与之相反,有状态服务在服务端保留之前请求的信息,用以处理当前请求,比如session等&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;虽然容器技术支持有状态服务功能（将部分数据随时进行备份，并且在创建一个新的有状态服务时，可以通过备份恢复这些数据，以达到数据持久化的目的），但考虑有状态的服务牵扯到存储卷挂载等问题很难在水平方向（服务器主机层面）上进行迁移，所以容器化场景下一个高效的架构模式是应用的进程都为无状态且无共享，任何需要持久化的数据都存储在后端服务内，比如数据库。
一些互联网系统依赖于 “粘性 session”， 这是指将用户session中的数据缓存至某进程的内存中，并将同一用户的后续请求路由到同一个进程，但这样在代码重新部署、配置更改或环境变更时仍会导致数据丢失，与前面的服务后端保持松耦合关系原则匹配的建议方案是将Session 中的数据保存在诸如 Memcached 或 Redis 这样的带有过期时间的缓存中。&lt;/p&gt;

&lt;h3 id=&quot;通过多进程实现并发&quot;&gt;通过多进程实现并发&lt;/h3&gt;
&lt;p&gt;通过将业务分为多个进程运行（例如下图web进程来处理HTTP请求，work进程处理后台业务工作等），并且应用程序必须可以在多台物理机器间跨进程工作，可以使系统扩展变得十分轻松，按需增加对应分类的进程数量便可以做到并发性能的提升。
&lt;img src=&quot;https://github.com/ShoreCN/ShoreCN.github.io/blob/master/resource/12factor_process.jpg?raw=true&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;注意进程的设计应该符合之前提出的无状态、无共享的原则，这样在扩展时能减少服务器、环境等因素的影响。&lt;/p&gt;

&lt;h2 id=&quot;总结&quot;&gt;总结&lt;/h2&gt;
&lt;p&gt;云原生的第一个目标就是需要将产品应用与底层资源(服务器、系统等)进行解耦，开发者只需关注自己的核心业务即可。虽然现在的开发语言大多标榜着跨平台轻松移植，但实际开发过程中系统版本、底层组件、驱动、补丁等因素最终或多或少都会引发一些问题，所以将环境资源轻量级虚拟化的容器技术目前是最优秀的解决方案。
而为了充分利用容器化技术的优势，需要将我们的应用进行相应的模块化处理，微服务是一个目前业界使用率较高的架构方式，加上之前云原生的应用开发原则，应用可以灵活地做到跨云环境提供业务服务，垂直、水平的快速扩展。
&lt;img src=&quot;https://github.com/ShoreCN/ShoreCN.github.io/blob/master/resource/cloud_native_mindmap.jpg?raw=true&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;参考资料：&lt;br /&gt;
&lt;a href=&quot;https://12factor.net/&quot;&gt;The Twelve-Factor App&lt;/a&gt;&lt;br /&gt;
&lt;a href=&quot;http://dockone.io/article/2991&quot;&gt;云原生架构概述 - DockOne.io&lt;/a&gt;&lt;br /&gt;
&lt;a href=&quot;https://zhuanlan.zhihu.com/p/27196777&quot;&gt;云原生（CloudNative）将成为应用云化开发的主流方式 - 知乎&lt;/a&gt;&lt;br /&gt;
&lt;a href=&quot;https://cloud.tencent.com/developer/article/1156697&quot;&gt;云原生概念 - 云+社区 - 腾讯云&lt;/a&gt;&lt;/p&gt;

&lt;/blockquote&gt;</content><author><name>Shore</name></author><category term="云计算" /><summary type="html">传统开发模式困局 开发、测试、运维无法一体化，上线周期长 开发、测试、线上多套环境需要进行单独维护，容易造成不一致影响各阶段的工作开展，进而引发一些线上故障等问题</summary></entry><entry><title type="html">跨云平台实施方案预研</title><link href="http://localhost:4000/%E6%9E%B6%E6%9E%84/2018/11/20/Cross-Cloud-credible-analysis/" rel="alternate" type="text/html" title="跨云平台实施方案预研" /><published>2018-11-20T23:45:20+08:00</published><updated>2018-11-20T23:45:20+08:00</updated><id>http://localhost:4000/%E6%9E%B6%E6%9E%84/2018/11/20/Cross-Cloud-credible-analysis</id><content type="html" xml:base="http://localhost:4000/%E6%9E%B6%E6%9E%84/2018/11/20/Cross-Cloud-credible-analysis/">&lt;h2 id=&quot;基于k8s集群搭建&quot;&gt;基于K8S集群搭建&lt;/h2&gt;
&lt;p&gt;在不同云厂商或同个云厂商不同区域下创建K8S集群，每个K8S集群中独立运行代理程序，通过代理程序进行镜像部署、服务发现、负载均衡等功能。
然后再开发一套对K8S集群进行控制管理的系统作为后台，用来进行多个集群的资源管理调度。
优势：单个K8S集群下的功能可以利用之前拾云平台的现成代码
缺陷：对多个K8S集群的资源进行重新分配管理需要较巨大的开发工作量&lt;/p&gt;

&lt;p&gt;此方案工作量较大且重复。&lt;/p&gt;

&lt;h2 id=&quot;基于k8s联邦集群搭建&quot;&gt;基于K8S联邦集群搭建&lt;/h2&gt;
&lt;p&gt;使用K8S联邦集群的机制，利用K8S定义好的各类资源概念，省去了资源调度、集群间通信、等诸多开发工作量。
&lt;img src=&quot;https://github.com/ShoreCN/ShoreCN.github.io/blob/master/resource/%E8%B7%A8%E4%BA%91%E5%B9%B3%E5%8F%B0%E6%9E%B6%E6%9E%84%E6%A1%86%E6%9E%B6%E5%9B%BE.jpg?raw=true&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;可以将产品分为如下几个模块：&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;服务器适配：
这层主要内容是通过在云主机上常驻一个agent程序，对接我们管理平台的控制指令，并实时采集系统资源情况。
可以使用golang开发，具有语法简单性能较高，并且编译生成出服务代理agent程序之后，可以方便地部署在不同的云厂商主机上，不用关心跨厂商主机之间的环境差异等问题。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;管理平台：
通过将K8S的各类资源（Cluster、Deployment、ReplicasSet、Service、Ingress等），按照业务逻辑封装成我们自己的系统后台功能（集群管理、部署、冗余、服务、负载均衡等），通过沿用部分原有的后台代码，使用Python语言以Tornado为框架开发一套针对上述资源的RESTful API，可以沿用之前的后台框架设计。
&lt;img src=&quot;https://raw.githubusercontent.com/ShoreCN/ShoreCN.github.io/master/resource/TenCloud_Architecture.png&quot; alt=&quot;&quot; /&gt;
数据层、镜像、用户、Tornado框架部分可以沿用原有代码，重新开发业务服务部分的集群、部署、服务功能。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;管理平台操作界面：
通过对接后台提供的RESTful API，产品化设计后以前端形式展现管理界面。&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;精简版本&quot;&gt;精简版本&lt;/h3&gt;
&lt;p&gt;一个可以考虑的精简版本就是开发一个直接跑在云主机上的脚本式命令行管理系统。封装K8S联邦集群的部署过程、模板化整合K8S联邦集群底层的资源操作。
精简版本的开发工作量主要集中在：&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;业务模型梳理封装&lt;/li&gt;
  &lt;li&gt;命令解析转换&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;问题：&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;涉及一些用户第三方授权的操作（如GitHub OAuth认证），在命令行端可能难以实现&lt;/li&gt;
  &lt;li&gt;若想收集K8S以外的主机、任务等信息，还是需要实现一套响应网络请求的框架&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;基于现有产品搭建&quot;&gt;基于现有产品搭建&lt;/h2&gt;
&lt;h3 id=&quot;f5&quot;&gt;F5&lt;/h3&gt;
&lt;p&gt;行业领导者，F5 解决方案集成了以下各项：&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;公有云提供商，如 AWS、Microsoft Azure 和 Google Cloud Platform。&lt;/li&gt;
  &lt;li&gt;私有云和开源平台，如 OpenStack、VMware 和 OpenShift。&lt;/li&gt;
  &lt;li&gt;自动化工具包，因此您可以使用 Ansible、Puppet 或 HashiCorp 部署应用服务，在 DevOps 方法中为应用服务提供声明接口。&lt;/li&gt;
  &lt;li&gt;容器环境中的微服务（如 Kubernetes 和 Mesos），以便在东西向流量中启用应用服务。&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;a10-network&quot;&gt;A10 Network&lt;/h3&gt;
&lt;p&gt;跨云产品Harmony Controller 
Harmony平台的理念是随时随地实施策略，它将安全应用服务与内部部署环境、公有云、私有云和混合云衔接在一起。A10 Harmony 控制器具备虚拟化、SaaS 和物理规格，可用于管理多云环境下的安全应用交付服务，并集成多种配置和自动化服务。&lt;/p&gt;

&lt;h3 id=&quot;redhat&quot;&gt;RedHat&lt;/h3&gt;
&lt;p&gt;OpenShift容器平台
红帽 OpenShift 是一款开源容器应用程序平台，主要以 Docker 容器为基础，并采用 Kubernetes 容器集群管理进行编排。OpenShift可支持多种编程语言和服务，包括 Web 框架、数据库或与移动应用和外部后台的连接器。OpenShift 平台同时支持云原生、无状态的应用程序以及传统、有状态的应用
程序。
&lt;img src=&quot;https://github.com/ShoreCN/ShoreCN.github.io/blob/master/resource/Redhat_Openshift_Architecture.jpg?raw=true&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;rancher&quot;&gt;Rancher&lt;/h3&gt;
&lt;p&gt;Rancher为DevOps工程师提供了一个直观的用户界面来管理他们的服务容器，用户不需要深入了解Kubernetes概念就可以开始使用Rancher。 Rancher包含应用商店，支持一键式部署Helm和Compose模板。Rancher通过各种云、本地生态系统产品认证，其中包括安全工具，监控系统，容器仓库以及存储和网络驱动程序。
&lt;img src=&quot;https://github.com/ShoreCN/ShoreCN.github.io/blob/master/resource/Rancher_Architecture.jpg?raw=true&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;容器平台能同时对多个Kubernetes集群进行管理，但每个集群上运行的应用和其他集群隔离。&lt;/strong&gt;&lt;/p&gt;

&lt;h3 id=&quot;时速云&quot;&gt;时速云&lt;/h3&gt;
&lt;p&gt;企业级容器 PaaS 平台 TenxCloud Enterprise (TCE)
随着业务的发展，客户通常会使用多个不同的节点区域，甚至使用多个不同的基础云服务提供商。时速云的跨云管理解决方案通过创建私有集群，并添加不同IaaS的云主机，可一键将容器镜像发布到不同的IaaS平台上，屏蔽了不同IaaS厂商之间的底层异构问题，极大的降低了IT管理的复杂度，并可轻松实现应用在多个不同的IaaS之间的迁移、容灾以及热备份。&lt;/p&gt;

&lt;p&gt;方案特点：&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;一键创建私有Docker主机集群，并支持将云主机、物理机或者虚拟机等添加到私有集群。&lt;/li&gt;
  &lt;li&gt;通过时速云容器云平台统一管理多个不同IaaS上的应用。&lt;/li&gt;
  &lt;li&gt;利用容器技术屏蔽底层基础云服务之间的差异，实现应用在多个云之间的迁移、容灾及热备。&lt;/li&gt;
  &lt;li&gt;私有集群可以无缝使用容器部署、镜像下载、集群 API 等服务。&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;cloudify&quot;&gt;Cloudify&lt;/h3&gt;
&lt;p&gt;Cloudify是一个开源的云应用编排系统，可以让你的应用自动化在各种不同的云上方便地部署。Cloudify重点关注应用自动化，承担了部分业务自动化的工作。从云IaaS、PaaS、SaaS分层看，Cloudify是一个典型的面向应用编排自动化的PaaS平台。&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;参考资料&lt;br /&gt;
&lt;a href=&quot;https://www.redhat.com/cms/managed-files/cl-openshift-container-platform-3.5-datasheet-f7213kc-201704-a4-zh.pdf&quot;&gt;红帽 OPENSHIFT 容器平台 3.5&lt;/a&gt;&lt;br /&gt;
&lt;a href=&quot;https://www.jianshu.com/p/0622d7dbbaa7&quot;&gt;跨集群服务——如何利用Kubernetes 1.3实现跨区高可用 - 简书&lt;/a&gt;&lt;br /&gt;
&lt;a href=&quot;https://www.cnrancher.com/docs/rancher/v2.x/cn/overview/architecture/&quot;&gt;1 - 架构设计 | Rancher Labs&lt;/a&gt;&lt;br /&gt;
&lt;a href=&quot;https://blog.csdn.net/liukuan73/article/details/57082617?utm_source=blogxgwz3&quot;&gt;Cloudify：打通应用和基础架构自动化交付的“任督二脉” - liukuan73的专栏 - CSDN博客&lt;/a&gt;&lt;br /&gt;
&lt;a href=&quot;https://cloudify.co/&quot;&gt;Cloud &amp;amp; NFV Orchestration Based on TOSCA | Cloudify&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;</content><author><name>Shore</name></author><category term="架构" /><category term="云计算" /><summary type="html">基于K8S集群搭建 在不同云厂商或同个云厂商不同区域下创建K8S集群，每个K8S集群中独立运行代理程序，通过代理程序进行镜像部署、服务发现、负载均衡等功能。 然后再开发一套对K8S集群进行控制管理的系统作为后台，用来进行多个集群的资源管理调度。 优势：单个K8S集群下的功能可以利用之前拾云平台的现成代码 缺陷：对多个K8S集群的资源进行重新分配管理需要较巨大的开发工作量</summary></entry><entry><title type="html">跨云应用管理方案分析</title><link href="http://localhost:4000/%E6%9E%B6%E6%9E%84/2018/11/14/Platform-Aurchitecture-Cross-Cloud/" rel="alternate" type="text/html" title="跨云应用管理方案分析" /><published>2018-11-14T08:14:39+08:00</published><updated>2018-11-14T08:14:39+08:00</updated><id>http://localhost:4000/%E6%9E%B6%E6%9E%84/2018/11/14/Platform-Aurchitecture-Cross-Cloud</id><content type="html" xml:base="http://localhost:4000/%E6%9E%B6%E6%9E%84/2018/11/14/Platform-Aurchitecture-Cross-Cloud/">&lt;p&gt;跨云应用管理方案分析&lt;/p&gt;
&lt;h2 id=&quot;需求背景&quot;&gt;需求背景&lt;/h2&gt;
&lt;p&gt;随着技术的不断发展，越来越多的产品通过使用分布式架构，来解决服务器压力、数据可靠性的问题。为了更聚焦于自己的核心业务，许多企业会直接采用各类云厂商的服务或方案，降低门槛与成本。
但是对于那些业务灵活、数据敏感的企业及产品，使用的云计算服务就需要具有跨云厂商、灵活伸缩、快速切换等功能，保证在服务变更、功能调整等场景下业务受到较小冲击。&lt;/p&gt;

&lt;h2 id=&quot;相关技术&quot;&gt;相关技术&lt;/h2&gt;
&lt;h3 id=&quot;云计算&quot;&gt;云计算&lt;/h3&gt;
&lt;p&gt;云计算是一种信息技术（IT）模式，可以随时访问共享的可配置系统资源池和更高级的服务，这些服务通常可以通过互联网以最少的管理工作快速供应。云计算依靠资源共享来实现连贯性和规模经济，类似于公用事业。&lt;/p&gt;

&lt;h3 id=&quot;容器化&quot;&gt;容器化&lt;/h3&gt;
&lt;p&gt;容器占用资源少、部署快，每个应用可以被打包成一个容器镜像，每个应用与容器间成一对一关系也使容器有更大优势，使用容器可以在build或release的阶段，为应用创建容器镜像，因为每个应用不需要与其余的应用堆栈组合，也不依赖于生产环境基础结构，这使得从研发到测试、生产能提供一致环境。类似地，容器比虚机轻量、更“透明”，这更便于监控和管理。&lt;/p&gt;

&lt;h3 id=&quot;kubernetes&quot;&gt;Kubernetes&lt;/h3&gt;
&lt;p&gt;又称K8S，是Google开源的一个容器编排引擎，它支持自动化部署、大规模可伸缩、应用容器化管理。在生产环境中部署一个应用程序时，通常要部署该应用的多个实例以便对应用请求进行负载均衡。
在K8S中，我们可以创建多个容器，每个容器里面运行一个应用实例，然后通过内置的负载均衡策略，实现对这一组应用实例的管理、发现、访问，而这些细节都不需要运维人员去进行复杂的手工配置和处理。&lt;/p&gt;

&lt;h2 id=&quot;k8s集群&quot;&gt;K8S集群&lt;/h2&gt;
&lt;p&gt;K8S通过将多个主机组成集群并在上层再进行一次虚拟化，使得资源可以充分被利用并灵活地调度。
但是K8S集群是基于局域网建立的，所以若使用云主机的话，单个集群通常无法跨单个云厂商的多个区域，更不用说支持不同的云厂商。这个对于容灾、迁移、管理方面都是很大的制约。&lt;/p&gt;

&lt;h2 id=&quot;k8s联邦集群federation&quot;&gt;K8S联邦集群（Federation）&lt;/h2&gt;
&lt;p&gt;K8S提供了联邦集群的机制，将分布在多个区域或者多个云厂商的K8S集群整合成一个大的集群，统一管理与调度。&lt;/p&gt;

&lt;h3 id=&quot;优点&quot;&gt;优点&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;低延迟：让多个区域中的集群通过向距离它们最近的用户提供服务来最大限度地减少延迟。&lt;/li&gt;
  &lt;li&gt;故障隔离：最好有多个小型集群而不是一个单独的大型集群来进行故障隔离。&lt;/li&gt;
  &lt;li&gt;可扩展性：单个K8S集群具有可扩展性限制。&lt;/li&gt;
  &lt;li&gt;混合云：可以在不同的云提供商或本地数据中心上拥有多个群集。&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;实现&quot;&gt;实现&lt;/h3&gt;
&lt;p&gt;联邦集群可以轻松管理多个群集，它通过2个主要构件来实现：&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;跨群集同步资源：联邦可以使多个群集中的资源保持同步。 例如，可以确保多个群集中部署相同的程序。&lt;/li&gt;
  &lt;li&gt;跨群集发现：联邦提供了自动配置DNS服务器和对群集后端负载均衡的功能
&lt;img src=&quot;https://raw.githubusercontent.com/ShoreCN/ShoreCN.github.io/master/resource/k8s_federation_architecture.jpg&quot; alt=&quot;&quot; /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;影响&quot;&gt;影响&lt;/h3&gt;
&lt;p&gt;虽然联邦有很多有吸引力的用处，但也有一些注意事项：&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;增加网络带宽和成本：联邦控制台监视所有群集以确保当前状态符合预期。如果集群在云提供商或不同云提供商的不同区域(regions)运行，这可能会导致显著的网络成本。&lt;/li&gt;
  &lt;li&gt;减少跨群集隔离：联邦控制台中的错误可能影响所有群集。通过将联邦控制台中的逻辑保持最简，可以缓解这一问题，尽可能将大部分任务都交给K8S集群处理。 设计和实施也在安全方面做了很多考虑，并避免发生错误时多集群停机。&lt;/li&gt;
  &lt;li&gt;成熟度：联邦项目相对较新，不太成熟。 并非所有资源都可用，许多资源仍然是alpha状态。&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;跨云集群方案&quot;&gt;跨云集群方案&lt;/h2&gt;
&lt;h3 id=&quot;框架&quot;&gt;框架&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/ShoreCN/ShoreCN.github.io/master/resource/%E8%B7%A8%E4%BA%91%E5%B9%B3%E5%8F%B0%E6%9E%B6%E6%9E%84%E6%A1%86%E6%9E%B6%E5%9B%BE.jpg&quot; alt=&quot;&quot; /&gt;
简单框架原型，镜像功能独立于跨云，暂不列入框架图&lt;/p&gt;

&lt;h4 id=&quot;业务层&quot;&gt;业务层&lt;/h4&gt;
&lt;p&gt;K8S联邦集群可以管理的资源如框架图中所示，基于这些资源我们可以提供如下跨云功能：&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;集群管理&lt;/li&gt;
  &lt;li&gt;部署&lt;/li&gt;
  &lt;li&gt;服务&lt;/li&gt;
  &lt;li&gt;负载均衡&lt;/li&gt;
  &lt;li&gt;资源隔离&lt;/li&gt;
  &lt;li&gt;冗余/可靠性&lt;/li&gt;
  &lt;li&gt;安全&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;平台层&quot;&gt;平台层&lt;/h4&gt;
&lt;p&gt;利用K8S联邦集群的控制平台，运行一个核心管理平台，衔接业务层的各类操作和底层每个云集群的资源控制动作。&lt;/p&gt;

&lt;h4 id=&quot;服务器层&quot;&gt;服务器层&lt;/h4&gt;
&lt;p&gt;通过在各个云集群部署代理应用，获取各个云集群及下属云主机的控制权、资源、日志等等&lt;/p&gt;

&lt;h3 id=&quot;云主机&quot;&gt;云主机&lt;/h3&gt;
&lt;p&gt;云主机可以由客户采购+平台管理或者平台采购+平台管理：&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;客户采购+平台管理：由客户自行采购主机并管理核心业务，我们提供平台或工具进行管理，节省了集群采购开支并降低产品运营、数据相关的政策风险。但是至少需要获得用户的主机管理权限才能进行管理，对于一些业务敏感的企业可能会有抵触。&lt;/li&gt;
  &lt;li&gt;平台采购+平台管理：由我们采购大量主机并搭建平台，类似于给用户提供PaaS服务，增加了主机采购维护的成本和风险，好处在于具有完整的主机控制管理权，可以更自如地部署平台层面的资源调度管理功能。&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;规格&quot;&gt;规格&lt;/h3&gt;
&lt;p&gt;基于K8S v1.12版本，每个集群可以管理的规格如下：&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;不超过5000个nodes&lt;/li&gt;
  &lt;li&gt;不超过150000个pods&lt;/li&gt;
  &lt;li&gt;不超过300000个容器&lt;/li&gt;
  &lt;li&gt;每个node上不超过100个pods&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;市场竞品分析&quot;&gt;市场、竞品分析&lt;/h2&gt;
&lt;p&gt;应用交付(Application Delivery Controller，简称ADC)主要部署于数据中心的应用服务器前端，利用负载均衡、服务器卸载、压缩和缓存、链接复用和防火墙等技术，实现应用的可用性、高效性和安全性。
主要厂商：深信服、H3C、F5、A10、Radware和Array等&lt;/p&gt;

&lt;h3 id=&quot;f5&quot;&gt;F5&lt;/h3&gt;
&lt;p&gt;行业领导者，F5 解决方案集成了以下各项：&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;公有云提供商，如 AWS、Microsoft Azure 和 Google Cloud Platform。&lt;/li&gt;
  &lt;li&gt;私有云和开源平台，如 OpenStack、VMware 和 OpenShift。&lt;/li&gt;
  &lt;li&gt;自动化工具包，因此您可以使用 Ansible、Puppet 或 HashiCorp 部署应用服务，在 DevOps 方法中为应用服务提供声明接口。&lt;/li&gt;
  &lt;li&gt;容器环境中的微服务（如 Kubernetes 和 Mesos），以便在东西向流量中启用应用服务。&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;a10-network&quot;&gt;A10 Network&lt;/h3&gt;
&lt;h4 id=&quot;harmony-controller&quot;&gt;Harmony Controller&lt;/h4&gt;
&lt;p&gt;Harmony 平台的理念是随时随地实施策略，它将安全应用服务与内部部署环境、公有云、私有云和混合云衔接在一起。A10 Harmony 控制器具备虚拟化、SaaS 和物理规格，可用于管理多云环境下的安全应用交付服务，并集成多种配置和自动化服务。&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;参考资料：&lt;br /&gt;
&lt;a href=&quot;https://kubernetes.io/docs/concepts/cluster-administration/federation/&quot;&gt;Federation - Kubernetes&lt;/a&gt;&lt;br /&gt;
&lt;a href=&quot;https://kubernetes.io/docs/setup/cluster-large/&quot;&gt;Building Large Clusters - Kubernetes&lt;/a&gt;&lt;br /&gt;
&lt;a href=&quot;http://stor.51cto.com/art/201708/546850.htm&quot;&gt;A10 Networks来了，给你来点不一样的料 - 51CTO.COM&lt;/a&gt;&lt;br /&gt;
&lt;a href=&quot;http://cloud.it168.com/a2018/0619/3210/000003210034.shtml&quot;&gt;专访：多云应用交付新时代下的F5与中国-云计算专区&lt;/a&gt;&lt;br /&gt;
&lt;a href=&quot;http://network.51cto.com/act/f5/ADreport&quot;&gt;F5发布2018全球应用交付报告&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;</content><author><name>Shore</name></author><category term="Kubernetes" /><summary type="html">跨云应用管理方案分析 需求背景 随着技术的不断发展，越来越多的产品通过使用分布式架构，来解决服务器压力、数据可靠性的问题。为了更聚焦于自己的核心业务，许多企业会直接采用各类云厂商的服务或方案，降低门槛与成本。 但是对于那些业务灵活、数据敏感的企业及产品，使用的云计算服务就需要具有跨云厂商、灵活伸缩、快速切换等功能，保证在服务变更、功能调整等场景下业务受到较小冲击。</summary></entry><entry><title type="html">GitHub OAuth第三方登录</title><link href="http://localhost:4000/%E5%B7%A5%E5%85%B7/2018/08/12/GitHub-OAuth2-documents/" rel="alternate" type="text/html" title="GitHub OAuth第三方登录" /><published>2018-08-12T07:29:08+08:00</published><updated>2018-08-12T07:29:08+08:00</updated><id>http://localhost:4000/%E5%B7%A5%E5%85%B7/2018/08/12/GitHub-OAuth2-documents</id><content type="html" xml:base="http://localhost:4000/%E5%B7%A5%E5%85%B7/2018/08/12/GitHub-OAuth2-documents/">&lt;h2 id=&quot;github-oauth第三方登录&quot;&gt;GitHub OAuth第三方登录&lt;/h2&gt;
&lt;h2 id=&quot;前言&quot;&gt;前言&lt;/h2&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;OAuth分1.0、1.0A、2.0三个版本，前二者已经淘汰以下所介绍的都是指2.0版本。
本文将以第三方网站获取GitHub代码仓库为例，简述OAuth的运作流程。
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;oauth介绍&quot;&gt;OAuth介绍&lt;/h2&gt;
&lt;blockquote&gt;
  &lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;开放授权（OAuth）是一个开放标准，允许用户让第三方应用访问该用户在某一网站上存储的私密的资源（如照片，视频，联系人列表），而无需将用户名和密码提供给第三方应用  
  
OAuth允许用户提供一个令牌，而不是用户名和密码来访问他们存放在特定服务提供者的数据。每一个令牌授权一个特定的网站（例如，视频编辑网站)在特定的时段（例如，接下来的2小时内）内访问特定的资源（例如仅仅是某一相册中的视频）。这样，OAuth让用户可以授权第三方网站访问他们存储在另外服务提供者的某些特定信息，而非所有内容。  
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;  &lt;/div&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;https://zh.wikipedia.org/wiki/开放授权&lt;br /&gt;
——维基百科&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;oauth原理&quot;&gt;OAuth原理&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;https://github.com/ShoreCN/ShoreCN.github.io/blob/master/resource/1-OAuth2-Authorization-Code-Flow.png?raw=true&quot; alt=&quot;&quot; /&gt;
	我们把OAuth2的整个认证过程大致分为三个阶段。第一阶段主要是向用户取得授权许可，对应图中的第1、2、3步；第二阶段主要是申请访问令牌（access_token），对应图中的第4、5步；第三阶段就是使用access_token获取用户数据。&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;参考资料： &lt;a href=&quot;http://insights.thoughtworkers.org/attack-aim-at-oauth2/&quot;&gt;移花接木：针对OAuth2的攻击 – ThoughtWorks洞见&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;oauth应用实例&quot;&gt;OAuth应用实例&lt;/h2&gt;
&lt;p&gt;案例背景：
我们有这么一个应用——可以让用户绑定GitHub账号，然后使用仓库代码在线构建容器镜像的平台。所以我们要获取用户的GitHub仓库信息用于后续下载代码、构建容器等流程，此处我们就可以使用GitHub OAuth达到这个目的。&lt;/p&gt;

&lt;h4 id=&quot;在github上创建oauth-app&quot;&gt;在GitHub上创建OAuth App&lt;/h4&gt;
&lt;ol&gt;
  &lt;li&gt;打开 Setting &amp;gt; Developer setting &amp;gt; OAuth applications&lt;/li&gt;
  &lt;li&gt;点击 Register a new application&lt;/li&gt;
  &lt;li&gt;填入基本的app信息&lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;获取对应的client-id与client-secret&quot;&gt;获取对应的Client ID与Client Secret&lt;/h4&gt;
&lt;p&gt;OAuth应用创建完成之后，可以在应用信息页面查看到Client ID与Client Secret的值，二者是完成OAuth认证流程的关键。&lt;/p&gt;

&lt;h4 id=&quot;应用生成获取github授权的链接&quot;&gt;应用生成获取GitHub授权的链接&lt;/h4&gt;
&lt;p&gt;然后就来到了我们的应用与GitHub联系的关键点，在应用上可能就是一个简单的绑定GitHub账号的按钮，本质上是一串由关键参数组成的URL，下面我们详细说一下这个URL是如何生成的。&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;URL样例：
https://github.com/login/oauth/authorize?client_id=5448811562b83dadc3cf&amp;amp;scope=repo%2Cuser%3Aemail&amp;amp;redirect_uri=http%3A%2F%2Fcd.10.com%2Fapi%2Fgithub%2Foauth%2Fcallback%3Fredirect_url%3Dhttp%253A%252F%252Fwww.baidu.com%26uid%3D99
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;乍看之下，这个URL杂乱无章无从下手，但其实知晓了原理之后就很好理解了。
这个URL承载了GItHub OAuth认证的所有步骤，那么回到之前的原理解析，其实这个URL就包含了两个动作：&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;跳转到GitHub界面申请用户授权
https://github.com/login/oauth/authorize 
这是GitHub的认证API，传入的参数是client_id, scope, redirect_uri
    &lt;ul&gt;
      &lt;li&gt;client_id: 之前步骤中创建OAuth App的时候获取到的字段，传入此参数可以让GitHub知道到底是哪个App在申请用户的授权&lt;/li&gt;
      &lt;li&gt;scope: 需要向用户申请授予的权限，具体规则可参考 &lt;img src=&quot;https://developer.github.com/apps/building-oauth-apps/scopes-for-oauth-apps/&quot; alt=&quot;GitHub API Doc&quot; /&gt;&lt;/li&gt;
      &lt;li&gt;redirect_uri: 如果用户完成授权后则跳转到此页面，在这个页面的请求中GitHub会带上code参数，用于下一步获取token&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;申请访问令牌
在上一步拿到GitHub返回的code参数之后，我们就可以去获取具有访问GitHub权限的token了。
https://github.com/login/oauth/access_token
获取token的API，传入参数client_id, client_secret, code
    &lt;ul&gt;
      &lt;li&gt;client_id: 之前步骤中创建OAuth App的时候获取到的字段，传入此参数可以让GitHub知道到底是哪个App在使用用户的授权&lt;/li&gt;
      &lt;li&gt;client_secret: 之前步骤中创建OAuth App的时候获取到的字段，传入此参数可以让GitHub知道此次操作是否属于该App的合法使用者&lt;/li&gt;
      &lt;li&gt;code: GitHub提供给App的合法认证标志&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;从token API的返回值中取出参数access_token，用于我们后续去获取GtiHub资源。&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;获取GitHub资源
我们这里以获取用户下的仓库列表为例，演示如何使用OAuth token，。
https://api.github.com/user/repos
这是获取仓库的API，假若token值为123456789abc，那么在GET的请求头中加入&lt;code class=&quot;highlighter-rouge&quot;&gt;{‘Authorization’: ‘token 123456789abc’}&lt;/code&gt;，就可以拿到GitHub数据了。
返回值：
    &lt;ul&gt;
      &lt;li&gt;repos_url: 仓库地址&lt;/li&gt;
      &lt;li&gt;repos_name： 仓库名称&lt;/li&gt;
      &lt;li&gt;http_url ：仓库https地址&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;至此，我们就完成了使用OAuth授权、认证、访问资源的全流程，其他接口类似，只要有了token就有了权限可以完成所有资源获取的操作。&lt;/p&gt;</content><author><name>Shore</name></author><category term="GitHub" /><summary type="html">GitHub OAuth第三方登录 前言 OAuth分1.0、1.0A、2.0三个版本，前二者已经淘汰以下所介绍的都是指2.0版本。 本文将以第三方网站获取GitHub代码仓库为例，简述OAuth的运作流程。</summary></entry><entry><title type="html">Kubernetes部署WordPress项目相关yaml文件</title><link href="http://localhost:4000/%E5%AE%B9%E5%99%A8/2018/08/02/Kubernetes%E9%83%A8%E7%BD%B2WordPress%E9%A1%B9%E7%9B%AE%E7%9B%B8%E5%85%B3yaml%E6%96%87%E4%BB%B6/" rel="alternate" type="text/html" title="Kubernetes部署WordPress项目相关yaml文件" /><published>2018-08-02T23:31:30+08:00</published><updated>2018-08-02T23:31:30+08:00</updated><id>http://localhost:4000/%E5%AE%B9%E5%99%A8/2018/08/02/Kubernetes%E9%83%A8%E7%BD%B2WordPress%E9%A1%B9%E7%9B%AE%E7%9B%B8%E5%85%B3yaml%E6%96%87%E4%BB%B6</id><content type="html" xml:base="http://localhost:4000/%E5%AE%B9%E5%99%A8/2018/08/02/Kubernetes%E9%83%A8%E7%BD%B2WordPress%E9%A1%B9%E7%9B%AE%E7%9B%B8%E5%85%B3yaml%E6%96%87%E4%BB%B6/">&lt;h1 id=&quot;kubernetes部署wordpress项目相关yaml文件&quot;&gt;Kubernetes部署WordPress项目相关yaml文件&lt;/h1&gt;
&lt;h2 id=&quot;mysql&quot;&gt;MySQL&lt;/h2&gt;
&lt;h3 id=&quot;部署&quot;&gt;部署&lt;/h3&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;apiVersion: apps/v1
kind: Deployment
metadata:
  labels: &amp;amp;id001
    app: wordpress-demo-mysql
    app_id: '262'
    internal_name: wordpress-demo-mysql.wordpress-demo-mysql
  name: wordpress-demo-mysql.wordpress-demo-mysql
spec:
  replicas: 1
  selector:
    matchLabels: *id001
  template:
    metadata:
      labels: *id001
    spec:
      containers:
      - image: 47.75.159.100:5000/wordpress-demo/wordpress-demo-mysql-pure:5.7
        name: mysql
        ports:
        - containerPort: 3306
          name: mysql
          protocol: TCP
        env:
        - name: MYSQL_ROOT_PASSWORD
          value: sqsm1234
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;服务&quot;&gt;服务&lt;/h3&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;apiVersion: v1
kind: Service
metadata:
  labels:
    app_id: '261'
    internal_name: wordpress-demo.wordpress-demo-mysql
  name: wordpress-demo-mysql
spec:
  ports:
  - name: mysql
    port: 3306
    protocol: TCP
    targetPort: 3306
  selector:
    app: wordpress-demo-mysql
    app_id: '262'
  type: ClusterIP
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;wordpress&quot;&gt;WordPress&lt;/h2&gt;
&lt;h3 id=&quot;部署-1&quot;&gt;部署&lt;/h3&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;apiVersion: apps/v1
kind: Deployment
metadata:
  labels: &amp;amp;id001
    app: wordpress-demo-web
    app_id: '266'
    internal_name: wordpress-demo-web.wordpress-demo-web
  name: wordpress-demo-web.wordpress-demo-web
spec:
  replicas: 1
  selector:
    matchLabels: *id001
  template:
    metadata:
      labels: *id001
    spec:
      containers:
      - image: 47.75.159.100:5000/wordpress-web-apche/wordpress-web-apche:4.9.7
        name: web
        ports:
        - containerPort: 80
          name: http
          protocol: TCP
        env:
        - name: WORDPRESS_DB_HOST
          value: wordpress-demo-mysql
        - name: WORDPRESS_DB_PASSWORD
          value: sqsm1234
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;服务-1&quot;&gt;服务&lt;/h3&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;apiVersion: v1
kind: Service
metadata:
  labels:
    app_id: '261'
    internal_name: wordpress-demo.wordpress-demo-web
  name: wordpress-demo-web
spec:
  ports:
  - name: http
    port: 6767
    protocol: TCP
    targetPort: 80
  selector:
    app: wordpress-demo-web
    app_id: '266'
  type: ClusterIP
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;</content><author><name>Shore</name></author><category term="Kubernetes" /><summary type="html">Kubernetes部署WordPress项目相关yaml文件 MySQL 部署 apiVersion: apps/v1 kind: Deployment metadata: labels: &amp;amp;id001 app: wordpress-demo-mysql app_id: '262' internal_name: wordpress-demo-mysql.wordpress-demo-mysql name: wordpress-demo-mysql.wordpress-demo-mysql spec: replicas: 1 selector: matchLabels: *id001 template: metadata: labels: *id001 spec: containers: - image: 47.75.159.100:5000/wordpress-demo/wordpress-demo-mysql-pure:5.7 name: mysql ports: - containerPort: 3306 name: mysql protocol: TCP env: - name: MYSQL_ROOT_PASSWORD value: sqsm1234</summary></entry><entry><title type="html">修改Kubernetes的NodePort范围</title><link href="http://localhost:4000/%E5%AE%B9%E5%99%A8/2018/05/20/Modify-the-Kubernetes-NodePort-config/" rel="alternate" type="text/html" title="修改Kubernetes的NodePort范围" /><published>2018-05-20T06:46:49+08:00</published><updated>2018-05-20T06:46:49+08:00</updated><id>http://localhost:4000/%E5%AE%B9%E5%99%A8/2018/05/20/Modify-the-Kubernetes-NodePort-config</id><content type="html" xml:base="http://localhost:4000/%E5%AE%B9%E5%99%A8/2018/05/20/Modify-the-Kubernetes-NodePort-config/">&lt;h2 id=&quot;问题&quot;&gt;问题&lt;/h2&gt;
&lt;p&gt;在我们的实际使用过程中，如果想让服务能从外部访问，就会使用到NodePort服务类型。默认的选择了NodePort类型之后，端口是从30000-32767范围内随机分配，就算用户自行指定其他端口号，也会报如下错误：&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;The Service &quot;test&quot; is invalid: spec.ports[0].nodePort: Invalid value: 7712: provided port is not in the valid range. The range of valid ports is 30000-32767
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;而我们实际的应用场景中，肯定有需要指定端口号的情况，例如服务要绑定域名只能用默认的80端口，而Kubernetes这样的不灵活限制了我们的使用。好在只需要修改API Server一个配置就可以调整NodePort的范围。&lt;/p&gt;

&lt;h2 id=&quot;api-server简介&quot;&gt;API Server简介&lt;/h2&gt;
&lt;p&gt;API Server提供了Kubernetes各类资源对象的增删查改等HTTP Rest接口，是整个系统的数据总线和数据中心。&lt;/p&gt;

&lt;p&gt;kubernetes API Server的功能：&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;提供了集群管理的REST API接口(包括认证授权、数据校验以及集群状态变更)；&lt;/li&gt;
  &lt;li&gt;提供其他模块之间的数据交互和通信的枢纽（其他模块通过API Server查询或修改数据，只有API Server才直接操作etcd）;&lt;/li&gt;
  &lt;li&gt;是资源配额控制的入口；&lt;/li&gt;
  &lt;li&gt;拥有完备的集群安全机制.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;api-server启动参数说明&quot;&gt;API Server启动参数说明&lt;/h2&gt;
&lt;p&gt;下面是一些典型的API Server参数说明，其中service-node-port-range就是我们需要关注修改的配置字段了。&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;--etcd-servers：指定etcd服务的URL
--insecure-bind-address：apiserver绑定主机的非安全IP地址，设置0.0.0.0表示监听所有IP
--insecure-port：apiserver绑定主机的非安全端口号，默认为8080
--service-cluster-ip-range：Kubernetes集群中Service的虚拟IP地址段范围，以CIDR格式表示，该IP范围不能与物理机的真实IP段有重合
--service-node-port-range：Kubernetes集群中Service可映射的物理机端口范围，默认为30000-32767
--admission-control：Kubernetes集群的准入控制设置爱，个控制模块以插件的形式生效
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;修改方案&quot;&gt;修改方案&lt;/h2&gt;
&lt;p&gt;修改API Server的配置，配置文件路径/etc/kubernetes/manifests/kube-apiserver.yaml，新增一行–service-node-port-range=80-32767&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;spec:
  containers:
  - command:
    - kube-apiserver
    - --service-account-key-file=/etc/kubernetes/pki/sa.pub
    - --secure-port=6443
    - --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.crt
    - --requestheader-extra-headers-prefix=X-Remote-Extra-
    - --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname
    - --requestheader-username-headers=X-Remote-User
    - --requestheader-group-headers=X-Remote-Group
    - --service-cluster-ip-range=10.96.0.0/12
    - --tls-private-key-file=/etc/kubernetes/pki/apiserver.key
    - --proxy-client-key-file=/etc/kubernetes/pki/front-proxy-client.key
    - --insecure-port=0
    - --enable-bootstrap-token-auth=true
    - --allow-privileged=true
    - --requestheader-allowed-names=front-proxy-client
    - --advertise-address=172.31.59.193
    - --proxy-client-cert-file=/etc/kubernetes/pki/front-proxy-client.crt
    - --admission-control=NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota
    - --tls-cert-file=/etc/kubernetes/pki/apiserver.crt
    - --kubelet-client-certificate=/etc/kubernetes/pki/apiserver-kubelet-client.crt
    - --kubelet-client-key=/etc/kubernetes/pki/apiserver-kubelet-client.key
    - --client-ca-file=/etc/kubernetes/pki/ca.crt
    - --authorization-mode=Node,RBAC
    - --etcd-servers=https://127.0.0.1:2379
    - --etcd-cafile=/etc/kubernetes/pki/etcd/ca.crt
    - --etcd-certfile=/etc/kubernetes/pki/apiserver-etcd-client.crt
    - --etcd-keyfile=/etc/kubernetes/pki/apiserver-etcd-client.key
    - --service-node-port-range=80-32767
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;配置修改之后Kubernetes会自动重新加载生效，不需要我们再做其他操作了，现在新创建的服务可以使用80端口对外通信了。&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;参考资料&lt;br /&gt;
&lt;a href=&quot;https://www.cnblogs.com/Cherry-Linux/p/7841273.html&quot;&gt;二、安装并配置Kubernetes Master节点 - Federico - 博客园&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;#Kubernetes#&lt;/p&gt;</content><author><name>Shore</name></author><category term="Kubernetes" /><summary type="html">问题 在我们的实际使用过程中，如果想让服务能从外部访问，就会使用到NodePort服务类型。默认的选择了NodePort类型之后，端口是从30000-32767范围内随机分配，就算用户自行指定其他端口号，也会报如下错误： The Service &quot;test&quot; is invalid: spec.ports[0].nodePort: Invalid value: 7712: provided port is not in the valid range. The range of valid ports is 30000-32767</summary></entry></feed>